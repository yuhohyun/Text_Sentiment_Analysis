{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1434aa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "pretrained_model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f382ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = SentimentDataset(\n",
    "        texts=df['Sentence'].values,\n",
    "        labels=df['Emotion'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3da075b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Sentence Emotion\n",
      "0                          김상수 정인욱 보내라      중립\n",
      "1                     특검은 대한민국이 응원합니다.      행복\n",
      "2                 거기서는 정말 행복하길 바랍니다…ㅠㅠ      슬픔\n",
      "3                          마블리가 왜 여기..      놀람\n",
      "4               긍게 언제쯤 온다고?! 왜 말이 없나?!      분노\n",
      "...                                ...     ...\n",
      "38589               좀 무섭기도 하고 그런거예요 ㅠㅠ      슬픔\n",
      "38590  해외 장거리로 헤어져서 지금은 헤어진지 육개월정도 됐어요      슬픔\n",
      "38591       난 가족이랑 있을땐 오히려 폰 안하는데.....      중립\n",
      "38592                노트7 팔려고 못들어오게 하나바      공포\n",
      "38593   우리 시어머니평범한 50댄데 자꾸 이런글 퍼다날림...      중립\n",
      "\n",
      "[38594 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 라벨링\n",
    "filename = './dataset/encoded_dialog_dataset.csv'\n",
    "df = pd.read_csv(filename)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d7ec18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion\n",
      "0    21829\n",
      "2    11935\n",
      "1     4830\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 라벨 매핑\n",
    "label_mapping = {'놀람': 2, '행복': 2, '공포': 0, '분노': 0, '슬픔': 0, '혐오': 0, '중립': 1}\n",
    "df['Emotion'] = df['Emotion'].map(label_mapping)\n",
    "\n",
    "# 각 레이블 값의 개수 계산\n",
    "counts = df['Emotion'].value_counts()\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ea2881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Sentence  Emotion\n",
      "0                          김상수 정인욱 보내라        1\n",
      "1                     특검은 대한민국이 응원합니다.        2\n",
      "2                 거기서는 정말 행복하길 바랍니다…ㅠㅠ        0\n",
      "3                          마블리가 왜 여기..        2\n",
      "4               긍게 언제쯤 온다고?! 왜 말이 없나?!        0\n",
      "...                                ...      ...\n",
      "38589               좀 무섭기도 하고 그런거예요 ㅠㅠ        0\n",
      "38590  해외 장거리로 헤어져서 지금은 헤어진지 육개월정도 됐어요        0\n",
      "38591       난 가족이랑 있을땐 오히려 폰 안하는데.....        1\n",
      "38592                노트7 팔려고 못들어오게 하나바        0\n",
      "38593   우리 시어머니평범한 50댄데 자꾸 이런글 퍼다날림...        1\n",
      "\n",
      "[38594 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179c3e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Sentence  Emotion\n",
      "0                               가격대비! 좋아요! 음량도 크고! 구매하세요        2\n",
      "1                   화질도 괜찮고 타사 제품에 비해 길이가 1M정도 더 길어서 좋아요        2\n",
      "2      생각보다 상당히 빨리 받았네요. OO에서 직접 배송해주는거라 설치 기사가 직접 배송...        1\n",
      "3                               팬소음이 너무 심하고 업그레이드가 불편하네요        0\n",
      "4      대학입학한 딸이 쓰기 딱 적당합니다 고급사양이라고 하긴 뭐하지만 가성비로는 이보다 ...        2\n",
      "...                                                  ...      ...\n",
      "40403  단말기 연결하는거 어렵지 않고 잘 되서 좋아요  후불 지급방식으로 자동으로 결제되게...        2\n",
      "40404  단말기기가 작아서 차안 인테리어와 잘 어울려서 좋아요 디자인이 깔끔하고 세련되서 이...        2\n",
      "40405  자가등록도 간편해서 오래 안걸렸고 괜찮았네요 운전석 앞에 두었는데 인식 잘 되서 마...        2\n",
      "40406  충전카드를 구매하고 자가등록을 완료하는 과정이 간단해서 좋아요 디자인이 차안에 맞고...        2\n",
      "40407  화이트 색상이 세련되고 깔끔하고 이쁘네요  걸리적거리지 않아 편하네요 선이 없어 깔...        2\n",
      "\n",
      "[40408 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "additional_filename = './dataset/additional.csv'\n",
    "df_additional = pd.read_csv(additional_filename)\n",
    "print(df_additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68afcd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Sentence  Emotion\n",
      "0                                            김상수 정인욱 보내라        1\n",
      "1                                       특검은 대한민국이 응원합니다.        2\n",
      "2                                   거기서는 정말 행복하길 바랍니다…ㅠㅠ        0\n",
      "3                                            마블리가 왜 여기..        2\n",
      "4                                 긍게 언제쯤 온다고?! 왜 말이 없나?!        0\n",
      "...                                                  ...      ...\n",
      "40403  단말기 연결하는거 어렵지 않고 잘 되서 좋아요  후불 지급방식으로 자동으로 결제되게...        2\n",
      "40404  단말기기가 작아서 차안 인테리어와 잘 어울려서 좋아요 디자인이 깔끔하고 세련되서 이...        2\n",
      "40405  자가등록도 간편해서 오래 안걸렸고 괜찮았네요 운전석 앞에 두었는데 인식 잘 되서 마...        2\n",
      "40406  충전카드를 구매하고 자가등록을 완료하는 과정이 간단해서 좋아요 디자인이 차안에 맞고...        2\n",
      "40407  화이트 색상이 세련되고 깔끔하고 이쁘네요  걸리적거리지 않아 편하네요 선이 없어 깔...        2\n",
      "\n",
      "[79002 rows x 2 columns]\n",
      "Emotion\n",
      "2    36090\n",
      "0    25611\n",
      "1    17301\n",
      "Name: count, dtype: int64\n",
      "기존의 각 라벨 값의 개수:\n",
      " Emotion\n",
      "2    36090\n",
      "0    25611\n",
      "1    17301\n",
      "Name: count, dtype: int64\n",
      "\n",
      "새로운 데이터셋의 크기: (51903, 2)\n",
      "\n",
      "새로운 데이터셋의 각 라벨 값의 개수:\n",
      " Emotion\n",
      "2    17301\n",
      "0    17301\n",
      "1    17301\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_added = pd.concat([df, df_additional], ignore_index=False)\n",
    "print(df_added)\n",
    "\n",
    "# 각 레이블 값의 개수 계산\n",
    "counts = df_added['Emotion'].value_counts()\n",
    "print(counts)\n",
    "\n",
    "# 'Emotion' 열에 대한 각 라벨 값의 개수 계산\n",
    "counts = df_added['Emotion'].value_counts()\n",
    "print(\"기존의 각 라벨 값의 개수:\\n\", counts)\n",
    "\n",
    "# 가장 적은 라벨의 개수 찾기\n",
    "min_count = counts.min()\n",
    "\n",
    "# 각 라벨별로 min_count만큼 데이터를 무작위로 선택\n",
    "balanced_df_list = []\n",
    "for label in counts.index:\n",
    "    # 현재 라벨의 데이터만 필터링\n",
    "    df_filtered = df_added[df_added['Emotion'] == label]\n",
    "    # 무작위로 min_count만큼 샘플링\n",
    "    df_sampled = df_filtered.sample(n=min_count, random_state=1)\n",
    "    balanced_df_list.append(df_sampled)\n",
    "\n",
    "# 선택된 데이터를 합쳐 새로운 데이터프레임 만들기\n",
    "balanced_df = pd.concat(balanced_df_list, ignore_index=True)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\n새로운 데이터셋의 크기:\", balanced_df.shape)\n",
    "print(\"\\n새로운 데이터셋의 각 라벨 값의 개수:\\n\", balanced_df['Emotion'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c1c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 토큰 수: 39.52047087836927\n",
      "중앙값 토큰 수: 26.0\n",
      "최대 토큰 수: 618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 48712 (\\N{HANGUL SYLLABLE BIN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 47928 (\\N{HANGUL SYLLABLE MUN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 48324 (\\N{HANGUL SYLLABLE BYEOL}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 53664 (\\N{HANGUL SYLLABLE TO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 53360 (\\N{HANGUL SYLLABLE KEUN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 48516 (\\N{HANGUL SYLLABLE BUN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 54252 (\\N{HANGUL SYLLABLE PO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 문장들의 토큰 수를 계산해서, model_max_length값을 결정한다.\n",
    "import matplotlib.pyplot as plt\n",
    "token_counts = balanced_df['Sentence'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# 기본 통계 정보 출력\n",
    "print(\"평균 토큰 수:\", token_counts.mean())\n",
    "print(\"중앙값 토큰 수:\", token_counts.median())\n",
    "print(\"최대 토큰 수:\", token_counts.max())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(token_counts, bins=30)\n",
    "plt.title('문장별 토큰 수 분포')\n",
    "plt.xlabel('토큰 수')\n",
    "plt.ylabel('빈도')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc454d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.25, random_state=32)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=32)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "max_len = 256\n",
    "batch_size = 32\n",
    "\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, max_len, batch_size)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, max_len, batch_size)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, max_len, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9abdc7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Sentence  Emotion\n",
      "0                          김상수 정인욱 보내라        1\n",
      "1                     특검은 대한민국이 응원합니다.        2\n",
      "2                 거기서는 정말 행복하길 바랍니다…ㅠㅠ        0\n",
      "3                          마블리가 왜 여기..        2\n",
      "4               긍게 언제쯤 온다고?! 왜 말이 없나?!        0\n",
      "...                                ...      ...\n",
      "38589               좀 무섭기도 하고 그런거예요 ㅠㅠ        0\n",
      "38590  해외 장거리로 헤어져서 지금은 헤어진지 육개월정도 됐어요        0\n",
      "38591       난 가족이랑 있을땐 오히려 폰 안하는데.....        1\n",
      "38592                노트7 팔려고 못들어오게 하나바        0\n",
      "38593   우리 시어머니평범한 50댄데 자꾸 이런글 퍼다날림...        1\n",
      "\n",
      "[38594 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d7db6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuhohyun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tf/KoELECTRA/wandb/run-20240525_101747-1wg8vtm7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yuhohyun/KoELECTRA_Emotion_Classification/runs/1wg8vtm7' target=\"_blank\">2024-05-25(5)(dropout=0.1)</a></strong> to <a href='https://wandb.ai/yuhohyun/KoELECTRA_Emotion_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yuhohyun/KoELECTRA_Emotion_Classification' target=\"_blank\">https://wandb.ai/yuhohyun/KoELECTRA_Emotion_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yuhohyun/KoELECTRA_Emotion_Classification/runs/1wg8vtm7' target=\"_blank\">https://wandb.ai/yuhohyun/KoELECTRA_Emotion_Classification/runs/1wg8vtm7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# init\n",
    "wandb.init(project='KoELECTRA_Emotion_Classification', \n",
    "           group='yuhohyun',\n",
    "           name='2024-05-25(5)(dropout=0.1)',\n",
    "           notes='Epoch = 20, max_len = 256, batch_size = 32, lr=7e-7',\n",
    "           tags='T',\n",
    "           entity='yuhohyun')\n",
    "\n",
    "# config\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 7e-7,\n",
    "  \"epochs\": 20,\n",
    "  \"batch_size\": 32,\n",
    "  \"max_len\" : 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e36f4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torchmetrics import Precision, Recall, F1Score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e341f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerModel(nn.Module):\n",
    "    def __init__(self, model, num_labels, num_cls_vector):\n",
    "        super(CustomTransformerModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.num_cls_vector = num_cls_vector  # 이 줄을 추가하여 num_cls_vector를 클래스 변수로 저장\n",
    "\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(num_cls_vector, num_labels)  # load and initialize weights\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        # Extract outputs from the body\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Add custom layers\n",
    "        sequence_output = self.dropout(outputs[0])  # outputs[0]=last hidden state\n",
    "        # 제공할 최종 logits 계산\n",
    "        logits = self.classifier(sequence_output[:, 0, :].view(-1, self.num_cls_vector))  # 여기서 self.num_cls_vector를 사용\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8badd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da3e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_corrects, train_steps = 0.0, 0, 0\n",
    "        train_precision, train_recall, train_f1 = Precision(average='macro', num_classes=3, task='multiclass').to(device), Recall(average='macro', num_classes=3, task='multiclass').to(device), F1Score(average='macro', num_classes=3, task='multiclass').to(device)\n",
    "        \n",
    "        # tqdm으로 학습 데이터 로더 감싸기\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\")\n",
    "        for batch in train_loader_tqdm:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            train_loss += loss.item() * inputs['input_ids'].size(0)\n",
    "            train_corrects += torch.sum(preds == labels.data)\n",
    "            train_steps += 1\n",
    "\n",
    "            # 메트릭 업데이트\n",
    "            train_precision.update(preds, labels)\n",
    "            train_recall.update(preds, labels)\n",
    "            train_f1.update(preds, labels)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_acc = train_corrects.double() / len(train_loader.dataset)\n",
    "        train_precision, train_recall, train_f1 = train_precision.compute(), train_recall.compute(), train_f1.compute()\n",
    "\n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_loss, val_corrects, val_steps = 0.0, 0, 0\n",
    "        val_precision, val_recall, val_f1 = Precision(average='macro', num_classes=3, task='multiclass').to(device), Recall(average='macro', num_classes=3, task='multiclass').to(device), F1Score(average='macro', num_classes=3, task='multiclass').to(device)\n",
    "\n",
    "        # tqdm으로 검증 데이터 로더 감싸기\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_tqdm:\n",
    "                inputs = {\n",
    "                    'input_ids': batch['input_ids'].to(device),\n",
    "                    'attention_mask': batch['attention_mask'].to(device)\n",
    "                }\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(**inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_loss += loss.item() * inputs['input_ids'].size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "                val_steps += 1\n",
    "\n",
    "                # 메트릭 업데이트\n",
    "                val_precision.update(preds, labels)\n",
    "                val_recall.update(preds, labels)\n",
    "                val_f1.update(preds, labels)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        val_precision, val_recall, val_f1 = val_precision.compute(), val_recall.compute(), val_f1.compute()\n",
    "\n",
    "\n",
    "        # 로깅\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss, \"train_acc\": train_acc.item(),\n",
    "            \"train_precision\": train_precision.item(), \"train_recall\": train_recall.item(), \"train_f1\": train_f1.item(),\n",
    "            \"val_loss\": val_loss, \"val_acc\": val_acc.item(),\n",
    "            \"val_precision\": val_precision.item(), \"val_recall\": val_recall.item(), \"val_f1\": val_f1.item()\n",
    "        })\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}')\n",
    "        print(f'Validation loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7efa3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "model = CustomTransformerModel(model=pretrained_model, num_labels=3, num_cls_vector=768)\n",
    "\n",
    "#손실 함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 실제 학습 실행\n",
    "epochs = 20\n",
    "\n",
    "# 최적화 함수 및 손실 함수\n",
    "optimizer = AdamW(model.parameters(), lr=7e-7, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * epochs # epochs는 학습할 총 에폭 수\n",
    "\n",
    "# 스케줄러는 선택사항\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 모델을 학습시키는 데 사용할 장비 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dda81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_validate_model(model, train_data_loader, val_data_loader, criterion, optimizer, scheduler, device, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0668c",
   "metadata": {},
   "source": [
    "### 학습한 model 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4ea035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장 경로 지정\n",
    "model_save_path = 'model.pth'\n",
    "\n",
    "# 모델의 state_dict 저장\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98227d",
   "metadata": {},
   "source": [
    "### 사용자가 입력한 text로 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"시간이 충분하셨을 텐데, 정말 많이 하셨네요.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "# 'token_type_ids' 제거\n",
    "if 'token_type_ids' in encoded_input:\n",
    "    del encoded_input['token_type_ids']\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델을 해당 디바이스로 이동\n",
    "model.to(device)\n",
    "\n",
    "# 입력 데이터를 해당 디바이스로 이동\n",
    "# 예를 들어, encoded_input이 입력 데이터라면\n",
    "encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=encoded_input['input_ids'], attention_mask=encoded_input['attention_mask'])\n",
    "\n",
    "# 예측 결과 처리\n",
    "# 직접 반환된 텐서를 logits 변수에 할당합니다.\n",
    "logits = outputs\n",
    "\n",
    "predicted_index = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "if predicted_index == 2:\n",
    "    print('positive')\n",
    "elif predicted_index == 1:\n",
    "    print('neutral')\n",
    "elif predicted_index == 0:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626c5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
